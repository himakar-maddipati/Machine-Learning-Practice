{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMM_MATH.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import matplotlib.animation as ani\n",
        "import matplotlib.cm as cmx\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "from PIL import Image\n",
        "from sklearn import datasets\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "DTXxSKHZwt3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now implement the Gaussian density function. Ahem... I know you can use numpy functions for that, but I believe it is actually interesting to see how things work internally. Our aim is to create a function that implements this:\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "p(\\mathbf x | \\mathbf\\mu, \\mathbf\\Sigma) = \\frac 1 {({2\\pi})^{n/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac 1 2 (\\mathbf x -\\mathbf\\mu)^T\\mathbf\\Sigma^{-1}(\\mathbf x -\\mathbf\\mu)\\right)\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "_SLWdwMHxAYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x is input with three columns\n",
        "x0 = np.array([[0.05, 1.413, 0.212], [0.85, -0.3, 1.11], [11.1, 0.4, 1.5], [0.27, 0.12, 1.44], [88, 12.33, 1.44]])\n",
        "#mu=mean with three arrays\n",
        "mu = np.mean(x0, axis=0)\n",
        "print(\"\\n Mean \\n\",mu)\n",
        "cov = np.dot((x0 - mu).T, x0 - mu) / (x0.shape[0] - 1)\n",
        "print(\"\\n Covaiance \\n\",cov)\n",
        "\n",
        "n = x0.shape[1]\n",
        "diff = (x0 - mu).T\n",
        "y = np.diagonal(1 / ((2 * np.pi) ** (n / 2) * np.linalg.det(cov) ** 0.5) * np.exp(-0.5 * np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff))).reshape(-1, 1)\n",
        "print(\"\\n This Gaussain distribution is for three dimensions\\n\")\n",
        "print(\"\\n probability= \\n\",y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G91e_YVwnDw",
        "outputId": "6e85dcb3-ae83-4af8-b4d7-7de58374900b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Mean \n",
            " [20.054   2.7926  1.1404]\n",
            "\n",
            " Covaiance \n",
            " [[1.46429833e+03 2.02328512e+02 7.59124800e+00]\n",
            " [2.02328512e+02 2.88241988e+01 6.42787700e-01]\n",
            " [7.59124800e+00 6.42787700e-01 2.92920800e-01]]\n",
            "\n",
            " This Gaussain distribution is for three dimensions\n",
            "\n",
            "\n",
            " probability= \n",
            " [[0.00159853]\n",
            " [0.00481869]\n",
            " [0.00276259]\n",
            " [0.0014309 ]\n",
            " [0.00143998]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For the purposes of this exercise, we will be using the Iris dataset, which is probably already familiar to you. We can easily obtain it by using the __load_iris__ function provided by sklearn:"
      ],
      "metadata": {
        "id": "LT9dQksuW0ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "X[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_8vryFgWr4D",
        "outputId": "b9287a23-c657-44d1-dfaf-8a1b84f1679e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 1__\n",
        "\n",
        "This is the initialization step of the GMM. At this point, we must initialise our parameters $\\pi_k$, $\\mu_k$, and $\\Sigma_k$. In this case, we are going to use the results of KMeans as an initial value for $\\mu_k$, set $\\pi_k$ to one over the number of clusters and $\\Sigma_k$ to the identity matrix. We could also use random numbers for everything, but using a sensible initialisation procedure will help the algorithm achieve better results."
      ],
      "metadata": {
        "id": "y4zMe24HWjNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters=3\n",
        "clusters = []\n",
        "idx = np.arange(X.shape[0])\n",
        "    \n",
        "# We use the KMeans centroids to initialise the GMM\n",
        "    \n",
        "kmeans = KMeans(n_clusters).fit(X)\n",
        "mu_k = kmeans.cluster_centers_\n",
        "    \n",
        "for i in range(n_clusters):\n",
        "    clusters.append({\n",
        "            'pi_k': 1.0 / n_clusters,\n",
        "            'mu_k': mu_k[i],\n",
        "            'cov_k': np.identity(X.shape[1], dtype=np.float64)\n",
        "        })\n",
        "    \n",
        "print(\"\\n Clusters\\n\",type(clusters))\n",
        "#print(\"\\n Clusters\\n\",clusters)\n",
        "#print(*clusters,sep='\\n')\n",
        "\n",
        "for elem in clusters:\n",
        "    print(\"\\n \\n\",elem )\n"
      ],
      "metadata": {
        "id": "twvw7L-7798u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88d78e93-b406-4fe9-d675-70ab62e8e6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Clusters\n",
            " <class 'list'>\n",
            "\n",
            " \n",
            " {'pi_k': 0.3333333333333333, 'mu_k': array([5.9016129 , 2.7483871 , 4.39354839, 1.43387097]), 'cov_k': array([[1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1.]])}\n",
            "\n",
            " \n",
            " {'pi_k': 0.3333333333333333, 'mu_k': array([5.006, 3.428, 1.462, 0.246]), 'cov_k': array([[1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1.]])}\n",
            "\n",
            " \n",
            " {'pi_k': 0.3333333333333333, 'mu_k': array([6.85      , 3.07368421, 5.74210526, 2.07105263]), 'cov_k': array([[1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1.]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 (Expectation step)**\n",
        "\n",
        "We should now calculate $\\gamma(z_{nk})$. We can achieve this by means of the following expression:\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\gamma{(z_{nk})}=\\frac {\\pi_k\\mathcal N(\\mathbf x_n| \\mathbf\\mu_k, \\mathbf\\Sigma_k)}{\\sum_{j=1}^K\\pi_j\\mathcal N(\\mathbf x_n| \\mathbf\\mu_j, \\mathbf\\Sigma_j)}\n",
        "\\end{equation}\n",
        "\n",
        "For convenience, we just calculate the denominator as a sum over all terms in the numerator, and then assign it to a variable named __totals__"
      ],
      "metadata": {
        "id": "zgI-jvvJYJ-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian(X, mu, cov):\n",
        "    n = X.shape[1]\n",
        "    diff = (X - mu).T\n",
        "    return np.diagonal(1 / ((2 * np.pi) ** (n / 2) * np.linalg.det(cov) ** 0.5) * np.exp(-0.5 * np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff))).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "eeRPZybSXuY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#global gamma_nk, totals\n",
        "N = X.shape[0]\n",
        "K = len(clusters)\n",
        "totals = np.zeros((N, 1), dtype=np.float64)\n",
        "gamma_nk = np.zeros((N, K), dtype=np.float64)\n",
        "\n",
        "for k, cluster in enumerate(clusters):\n",
        "    pi_k = cluster['pi_k']\n",
        "    mu_k = cluster['mu_k']\n",
        "    cov_k = cluster['cov_k']\n",
        "    #numpy.ravel flattens the array into 1-D array\n",
        "    prod=(pi_k * gaussian(X, mu_k, cov_k))\n",
        "    print(\"\\n prod shape\\n \",prod.shape)\n",
        "    #numpy.ravel flattens the array into 1-D array\n",
        "    prod2=prod.ravel()\n",
        "    print(\"\\n prod2 shape\\n \",prod2.shape)\n",
        "    #column is added to gamma_nk\n",
        "    gamma_nk[:, k] = prod2\n",
        "\n",
        "totals = np.sum(gamma_nk, 1)\n",
        "print(\"\\n totals shape \\n\",totals.shape)\n",
        "#since totals has only rows we use np.expand_dims to include column\n",
        "gamma_nk /= np.expand_dims(totals, 1)\n",
        "print(\"\\n gamma_nk shape \\n\",gamma_nk.shape)\n",
        "print(\"\\n gamma_nk  \\n\",type(gamma_nk))\n",
        "print(\"\\n gamma_nk  \\n\",gamma_nk[0:10,:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XodTW01fX8vq",
        "outputId": "d0bba409-1842-430f-b60a-52cc94979af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " prod shape\n",
            "  (150, 1)\n",
            "\n",
            " prod2 shape\n",
            "  (150,)\n",
            "\n",
            " prod shape\n",
            "  (150, 1)\n",
            "\n",
            " prod2 shape\n",
            "  (150,)\n",
            "\n",
            " prod shape\n",
            "  (150, 1)\n",
            "\n",
            " prod2 shape\n",
            "  (150,)\n",
            "\n",
            " totals shape \n",
            " (150,)\n",
            "\n",
            " gamma_nk shape \n",
            " (150, 3)\n",
            "\n",
            " gamma_nk  \n",
            " <class 'numpy.ndarray'>\n",
            "\n",
            " gamma_nk  \n",
            " [[9.97084205e-01 2.91301279e-03 2.78180471e-06]\n",
            " [9.96578648e-01 3.41905629e-03 2.29549986e-06]\n",
            " [9.98135034e-01 1.86400069e-03 9.65448472e-07]\n",
            " [9.96724020e-01 3.27402499e-03 1.95516665e-06]\n",
            " [9.97508245e-01 2.48952138e-03 2.23377617e-06]\n",
            " [9.91157572e-01 8.82072077e-03 2.17074167e-05]\n",
            " [9.97753409e-01 2.24521378e-03 1.37672525e-06]\n",
            " [9.96177919e-01 3.81840714e-03 3.67382242e-06]\n",
            " [9.97657998e-01 2.34105547e-03 9.46925056e-07]\n",
            " [9.96195703e-01 3.80146566e-03 2.83102823e-06]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 (Maximization step):**\n",
        "\n",
        "Let us now implement the maximization step. Since $\\gamma(z_{nk})$ is common to the expressions for $\\pi_k$, $\\mu_k$ and $\\Sigma_k$, we can simply define:\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "N_k=\\sum_{n=1}^N\\gamma({z_{nk}})\n",
        "\\end{equation}\n",
        "\n",
        "And then we can calculate the revised parameters by using:\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\pi_k^*=\\frac {N_k} N\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\mu_k^*=\\frac 1 {N_k} \\sum_{n=1}^N\\gamma({z_{nk}})\\mathbf x_n\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\Sigma_k^*=\\frac 1 {N_k} \\sum_{n=1}^N\\gamma({z_{nk}})(\\mathbf x_n-\\mathbf\\mu_k)(\\mathbf x_n-\\mathbf\\mu_k)^T\n",
        "\\end{equation}\n",
        "\n",
        "Note: To calculate the covariance, we define an auxiliary variable __diff__ that contains $(x_n-\\mu_k)^T$."
      ],
      "metadata": {
        "id": "ydYW0UgjaRxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = float(X.shape[0])\n",
        "\n",
        "for k, cluster in enumerate(clusters):\n",
        "    gamma_k = np.expand_dims(gamma_nk[:, k], 1)\n",
        "    N_k = np.sum(gamma_k, axis=0)\n",
        "    \n",
        "    pi_k = N_k / N\n",
        "    mu_k = np.sum(gamma_k * X, axis=0) / N_k\n",
        "    cov_k = (gamma_k * (X - mu_k)).T @ (X - mu_k) / N_k\n",
        "    \n",
        "    cluster['pi_k'] = pi_k\n",
        "    cluster['mu_k'] = mu_k\n",
        "    cluster['cov_k'] = cov_k\n",
        "\n",
        "print(\"\\n Clusters\\n\",clusters[0])\n",
        "print(\"\\n Clusters\\n\",clusters[1])\n",
        "print(\"\\n Clusters\\n\",clusters[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofhexGttaTsf",
        "outputId": "622e5407-1f7c-48eb-8653-f6e342bac252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Clusters\n",
            " {'pi_k': array([0.34077089]), 'mu_k': array([5.01175962, 3.40296744, 1.51313818, 0.26794469]), 'cov_k': array([[ 0.12319189,  0.09038184,  0.0300872 ,  0.01556953],\n",
            "       [ 0.09038184,  0.16169368, -0.03585906, -0.01113107],\n",
            "       [ 0.0300872 , -0.03585906,  0.13239249,  0.04920678],\n",
            "       [ 0.01556953, -0.01113107,  0.04920678,  0.02930876]])}\n",
            "\n",
            " Clusters\n",
            " {'pi_k': array([0.36962874]), 'mu_k': array([5.96205682, 2.77674085, 4.48948813, 1.48307989]), 'cov_k': array([[0.26299967, 0.07845149, 0.22896555, 0.08978328],\n",
            "       [0.07845149, 0.09463116, 0.08024942, 0.04960538],\n",
            "       [0.22896555, 0.08024942, 0.41856938, 0.1970684 ],\n",
            "       [0.08978328, 0.04960538, 0.1970684 , 0.131168  ]])}\n",
            "\n",
            " Clusters\n",
            " {'pi_k': array([0.28960037]), 'mu_k': array([6.67030916, 3.00875952, 5.46588637, 1.93313508]), 'cov_k': array([[0.35572165, 0.06886176, 0.32344008, 0.07669752],\n",
            "       [0.06886176, 0.09716685, 0.06696857, 0.0486227 ],\n",
            "       [0.32344008, 0.06696857, 0.46920563, 0.15609039],\n",
            "       [0.07669752, 0.0486227 , 0.15609039, 0.12896748]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now determine the log-likelihood of the model. It is given by:\n",
        "\n",
        "\\begin{equation}\n",
        "\\large\n",
        "\\ln p(\\mathbf X)=\\sum_{n=1}^N\\ln\\sum_{k=1}^K\\pi_k\\mathcal N(\\mathbf x_n|\\mu_k,\\Sigma_k)\n",
        "\\end{equation}\n",
        "\n",
        "However, the second summation has already been calculated in the __expectation_step__ function and is available in the __totals__ variable. So we just make use of it."
      ],
      "metadata": {
        "id": "G2gddaghbDHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_likelihoods = np.log(totals)\n",
        "sample_likelihoods_sum=np.sum(sample_likelihoods)\n",
        "print(\"\\n sample_likelihoods shape \\n \",sample_likelihoods.shape)\n",
        "print(\"\\n sample_likelihoods \\n \",sample_likelihoods)\n",
        "\n",
        "print(\"\\n sample_likelihoods_sum \\n \",sample_likelihoods_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O3GK7n2bE-0",
        "outputId": "ba167e9b-dffa-4e60-ea23-4e4fd25a4239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " sample_likelihoods shape \n",
            "  (150,)\n",
            "\n",
            " sample_likelihoods \n",
            "  [-4.78143637 -4.8711292  -4.85948971 -4.90907506 -4.78966156 -4.99467467\n",
            " -4.8583073  -4.77272702 -5.09801167 -4.84134487 -4.88628665 -4.80226568\n",
            " -4.89697316 -5.19061779 -5.28638908 -5.49554203 -4.98560141 -4.78146846\n",
            " -5.10311906 -4.84656104 -4.87192936 -4.82402424 -4.97881642 -4.83608639\n",
            " -4.88264386 -4.86982852 -4.78965261 -4.79327672 -4.79313883 -4.85326313\n",
            " -4.8546983  -4.85801432 -5.02782603 -5.19475756 -4.83126494 -4.83393465\n",
            " -4.91003946 -4.80533057 -5.06212185 -4.77676857 -4.78931331 -5.54991948\n",
            " -4.99672941 -4.84145336 -4.9427246  -4.88702781 -4.8542772  -4.88346865\n",
            " -4.85226936 -4.78249845 -4.85073253 -4.6388231  -4.68230036 -4.9673291\n",
            " -4.57764834 -4.64437717 -4.60615079 -5.76823049 -4.65926844 -5.06772478\n",
            " -5.83571131 -4.6613486  -5.01178888 -4.53493888 -5.04925456 -4.7690772\n",
            " -4.67785817 -4.81736981 -4.7403121  -4.94919443 -4.61819795 -4.74779653\n",
            " -4.57467043 -4.59231281 -4.68025415 -4.70293444 -4.67276167 -4.53944081\n",
            " -4.55274034 -5.18168639 -5.07222269 -5.16618482 -4.83454979 -4.56138428\n",
            " -4.78608776 -4.71128792 -4.64406581 -4.75259739 -4.79413929 -4.89181557\n",
            " -4.78136641 -4.5557575  -4.79817659 -5.74490637 -4.74129566 -4.75007463\n",
            " -4.70999261 -4.6374067  -5.71016817 -4.73872521 -4.92179459 -4.65601322\n",
            " -4.70118853 -4.57510933 -4.6316815  -5.39632805 -5.25168874 -5.00894225\n",
            " -4.72330516 -5.07522886 -4.54433579 -4.52907364 -4.55287231 -4.76905784\n",
            " -4.86498716 -4.62982439 -4.52062362 -5.8488825  -5.92575603 -4.75654529\n",
            " -4.65664779 -4.75056495 -5.6068393  -4.5110051  -4.61256017 -4.80475018\n",
            " -4.50801986 -4.51904624 -4.58984909 -4.78329104 -4.9566487  -5.78362956\n",
            " -4.61541645 -4.51986572 -4.77291689 -5.18119646 -4.78377457 -4.53796926\n",
            " -4.54278893 -4.57802081 -4.64859904 -4.68158541 -4.65601322 -4.69139826\n",
            " -4.74577272 -4.6096236  -4.58022609 -4.50710274 -4.74908565 -4.58843307]\n",
            "\n",
            " sample_likelihoods_sum \n",
            "  -727.7880991223876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gmm = GaussianMixture(n_components=n_clusters, max_iter=50).fit(X)\n",
        "gmm_scores = gmm.score_samples(X)\n",
        "\n",
        "print('Means by sklearn:\\n', gmm.means_)\n",
        "print('Means by our implementation:\\n', np.array([cluster['mu_k'].tolist() for cluster in clusters]))\n",
        "print('Scores by sklearn:\\n', gmm_scores[0:20])\n",
        "print('Scores by our implementation:\\n', sample_likelihoods.reshape(-1)[0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vciBXImWcYV8",
        "outputId": "16a6c53d-990a-4c62-ac75-c48650728913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Means by sklearn:\n",
            " [[5.91697517 2.77803998 4.20523542 1.29841561]\n",
            " [5.006      3.428      1.462      0.246     ]\n",
            " [6.54632887 2.94943079 5.4834877  1.98716063]]\n",
            "Means by our implementation:\n",
            " [[5.01175962 3.40296744 1.51313818 0.26794469]\n",
            " [5.96205682 2.77674085 4.48948813 1.48307989]\n",
            " [6.67030916 3.00875952 5.46588637 1.93313508]]\n",
            "Scores by sklearn:\n",
            " [ 1.57050082  0.73787138  1.14436656  0.92913238  1.411028   -0.09451903\n",
            "  0.05266884  1.62442195  0.27082378  0.16706624  0.83489877  0.77168582\n",
            "  0.29597841 -1.79224582 -3.41557928 -2.10529279 -1.12995447  1.47503579\n",
            " -0.84612536  0.97699215]\n",
            "Scores by our implementation:\n",
            " [-4.78143637 -4.8711292  -4.85948971 -4.90907506 -4.78966156 -4.99467467\n",
            " -4.8583073  -4.77272702 -5.09801167 -4.84134487 -4.88628665 -4.80226568\n",
            " -4.89697316 -5.19061779 -5.28638908 -5.49554203 -4.98560141 -4.78146846\n",
            " -5.10311906 -4.84656104]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# MATH IN EM ALGORITHM\n",
        "\n"
      ],
      "metadata": {
        "id": "pd4b2LwEl4Nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Probabilities\n",
        "For iris dataset assume there are three classes\n",
        "\n",
        "For each of the 150 samples we can find probability as follows:\n",
        "\n",
        "$p(x) =p(x|G_1)P(G_1)+p(x|G_2)P(G_2)+p(x|G_3)P(G_3)$\n",
        "\n",
        "Where \n",
        "\n",
        " 1. $G_1,\\,G_2,\\,G_3$ are three clusters or groups or mixture components, \n",
        " 2. x is sample\n",
        " 3. $P(G_1),\\,P(G_2),\\,P(G_3)$ are priors ,cluster proportions or  mixture proportions\n",
        " 4. $p(x|G_1),\\,p(x|G_2),\\,p(x|G_3)$ are sample cluster densities or component densities\n",
        "\n",
        "$ \\therefore p(x) =\\Sigma^k_{i=1}p(x|G_i)P(G_i)\\, here\\,k=1,2,3$"
      ],
      "metadata": {
        "id": "ENb_0irGUArn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample can be written as $X={\\{x^t\\}}_t$ \n",
        "\n",
        "For Iris dataset t=1,2,3...150  and sample is ***iid=independent and identically distributed***\n",
        "\n",
        "##2.Assumption:\n",
        " The component densities $p(x|G_i)$ are multivariate Gaussian\n",
        "\n",
        " $\\therefore p(x|G_i)\\sim \\mathcal{N}(\\mu_i,Σ_i)$\n",
        "\n",
        " The parameters to be Estimated are \n",
        "\n",
        " $Φ={\\{p(x|G_i),\\mu_i,Σ_i\\}}^k_{i=1}$"
      ],
      "metadata": {
        "id": "ps4hFxRpXmb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Loglikelihood:\n",
        "\n",
        "$L(Φ|X) = log\\prod_tp(x^t |Φ)=\\sum_tlog\\sum^k_{i=1}p(x^t|G_i)P(G_i)$\n",
        "\n",
        "###procedure\n",
        "\n",
        "1. For each sample calculate k cluster probabilities Sum and take log\n",
        "2. Sum all t samples probabilities log calculated in step1"
      ],
      "metadata": {
        "id": "zE-3p8sJcLX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Expectation -Maximisation Algorithmn\n",
        "\n",
        "Unsupervised Iris dataset  problem involves two sets of random variables of which one,***X(inputs), is observable*** and the other, ***Z(clusters), is hidden.*** \n",
        "\n",
        "The goal of the algorithm is to find the parameter vector $Φ $ to maximize the likelihood of the joint distribution of X and Z, the complete likelihood $\\mathcal{L_c}(Φ|X,Z)$.\n",
        "\n",
        "Since the Z values are not observed, we cannot work directly with the\n",
        "complete data likelihood $\\mathcal{L_c} $;\n",
        "\n",
        "***1.Expectation***\n",
        "\n",
        "Instead, we work with its expectation $E$ given $X$and the current parameter values $Φ^l$ , where ***l indexes iteration***.\n",
        "\n",
        "This is the expectation (E) step of the algorithm.\n",
        "\n",
        "E-step : $E(Φ|Φ^l) = E[Lc(Φ|X,Z)|X,Φ^l]$\n",
        "\n",
        "***2.Maximisation***\n",
        "\n",
        " Then in the maximization\n",
        "(M) step, we look for the new parameter values, $Φ^{l+1}$, that maximize this\n",
        "\n",
        "M-step :$ Φ^{l+1}= argmax_ΦE(Φ|Φ^l)$\n",
        "\n",
        "To find argmax we perform number of iterations(also called as epochs)\n",
        "\n",
        "***Procedure***\n",
        "1. E step :estimate Clusters(Z) for each sample\n",
        "2. M step :From estimated Clusters(Z) in step1 we find cluster probabilities of Samples\n",
        "3. Iterate 1 and 2 steps number of times\n",
        "\n",
        "***EM  Algorithmn Similarities with K-Means***\n",
        "1. E step :Estimate Clusters based on centroids\n",
        "2. M step :From estimated Clusters  in step1 we find means of Clusters samoples\n",
        "3. Iterate 1 and 2 steps number of times\n",
        "\n"
      ],
      "metadata": {
        "id": "JB1SevDXfM1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Derivation of Expectation-Step formula\n",
        "\n",
        "***I. Find Joint density of X and Z***\n",
        "\n",
        "We define a vector of ***indicator variables*** $z^t = \\{z^t_1, . . . , z^t_k\\}$\n",
        "\n",
        " where $z^t_i= 1$ if $x^t$ belongs to cluster $G_i $, and 0 otherwise.\n",
        "\n",
        " Example:\n",
        " 1. For iris dataset samples=150,clusters=3\n",
        " 2. So no of Indicator varaibles=150X3\n",
        "\n",
        "\n",
        " ***z is a multinomial distribution*** from k categories with prior probabilities ***$π_i $***, shorthand for $P(G_i)$\n",
        "\n",
        " Then\n",
        "$P(z^t) =\\prod ^k_{i=1}π^{z^t_i}_i $\n",
        "\n",
        "The likelihood of an observation $x^t$ is equal to its probability specified by\n",
        "the component that generated it:\n",
        "\n",
        "$p(x^t |z^t) =\\prod^k_{i=1}p_i(x^t)^{z^t_i}$\n",
        "\n",
        "$p_i(x^t)$ is shorthand for $p(x^t|G_i)$\n",
        "\n",
        " ***The joint density is***\n",
        "\n",
        "$p(x^t , z^t) = P(z^t)p(x^t |z^t)$ ....(1)\n",
        "\n",
        "\n",
        "***II. Likelihood of the sample X***\n",
        "\n",
        "and the complete data likelihood of the iid sample X is\n",
        "\n",
        "$Lc(Φ|X,Z) $=log of product of joint probability(X,Z)\n",
        "\n",
        "$Lc(Φ|X,Z) = log\\prod_t p(x^t , z^t |Φ)$\n",
        "\n",
        "$Lc(Φ|X,Z)=\\sum_t log\\ p(x^t , z^t |Φ)$ ***(used in colab calculations)***\n",
        "\n",
        "substitute ***joint density of X,Z*** from  equ...(1)\n",
        "\n",
        "$Lc(Φ|X,Z)=\\sum_t log\\ P(z^t |Φ) + log\\ p(x^t|z^t,Φ)$\n",
        "\n",
        "After   bringing  out common indicator variable $z^t_i$\n",
        "\n",
        "$Lc(Φ|X,Z)=\\sum_t \\sum_iz^t_i [logπ_i + log \\ p_i(x^t |Φ)]$......(2)\n",
        "\n",
        "\n",
        "***III. Estimation Step***\n",
        "\n",
        "***IIIa.Parameter Estimation***\n",
        "\n",
        "$E(Φ|Φ^l) ≡ E[log P(X,Z)|X,Φ^l]$\n",
        "\n",
        "$E(Φ|Φ^l)= E[Lc(Φ|X,Z)|X,Φ^l)]$\n",
        "\n",
        "substituting equ...(2)\n",
        "\n",
        "$E(Φ|Φ^l)=\\sum_t\\sum_iE[z^t_i|X,Φ^l][logπ_i + log \\ p_i(x^t |Φ^l)]$ ....(3)\n",
        "\n",
        "***IIIB.Estimation of Z***\n",
        "\n",
        "$E[z^t_i|X,Φ^l] = E[z^t_i|x^t ,Φ^l]$  where $x^t$ are i.i.d\n",
        "\n",
        "$= P(z^t_i= 1|x^t ,Φ^l) \\ $ where $ \\ z^t_i\\ $ is a 0/1 random variable\n",
        "\n",
        "$=\\frac{p(x^t |z^t_i= 1,Φ^l)P(z^t_i= 1|Φ^l)}{p(x^t |Φ^l)} \\ $ ***Bayes’ rule***\n",
        "\n",
        "$= \\frac{p_i(x^t |Φ^l)π_i}{\\Sigma_j p_j (x^t |Φ^l)π_j}$\n",
        "\n",
        "\n",
        "$= \\frac{p(x^t|G_i ,Φ^l)P(G_i)}{\\sum_j p(x^t|G_j ,Φ^l)P(G_j} $\n",
        " \n",
        "\n",
        "$= P(G_i|x^t ,Φ^l) ≡ h^t_i$ ....(4)\n",
        "\n",
        "We see that the expected value of the hidden variable, $E[z^t_\n",
        "i ]$, is the posterior probability that $x^t$ is generated by component $G_i$ .\n",
        "\n",
        " Because this is a probability, it is between 0 and 1 and is a “soft” label, as opposed to the 0/1 “hard” label of k-means.\n",
        "\n",
        "\n",
        " ***IV.Maximisation Step***\n",
        "\n",
        " We maximize E to get the next set of parameter values $Φ^{l+1}$:\n",
        "\n",
        "\n",
        "$Φ^{l+1} = argmax_ΦE(Φ|Φ^l)$\n",
        "\n",
        "From substituting equ...(4) in (3)\n",
        "\n",
        "$E(Φ|Φ^l) =\\sum_t\\sum_i h^t_i[log π_i + log p_i(x^t |Φ^l)]$\n",
        "\n",
        "***the above is parameter Estimate equation***\n",
        "\n",
        "$=\\sum_t\\sum_ih^t_i log π_i +\\sum_t\\sum_ih^t_ilog \\ p_i(x^t |Φ^l)$\n",
        "\n",
        "\n",
        "The second term is independent of $π_i$ and using the constraint that\n",
        "$\\sum_i π_i = 1$ as the Lagrangian, we solve for($∇π_i$=differentiate w.r.t $\\pi_i$)\n",
        "\n",
        "\n",
        "$∇π_i\\sum_t\\sum_i h^t_i logπ_i − λ(\\sum_iπ_i − 1) = 0$\n",
        "\n",
        "and get\n",
        "\n",
        "\n",
        "$π_i =\\frac{\\sum_t h^t_i}{N}$  ..........(5)\n",
        "\n",
        "\n",
        "Similarly, the first term of parameter Estimate equation is independent of the components and can be dropped while estimating the parameters of the components.\n",
        "We solve for($∇_Φ$=differentiate w.r.t $Φ$)\n",
        "\n",
        "$∇_Φ\\sum_t\\sum_i h^t_i \\log p_i(x^t |Φ) = 0$\n",
        "\n",
        "If we assume Gaussian components, $ˆp_i(x^t |Φ) ∼ N(m_i , S_i),$ the M-step\n",
        "is\n",
        "\n",
        "By substituting gaussain equation in equation above we get\n",
        "\n",
        "$∇_{m_i}\\sum_t\\sum_i h^t_i \\log p_i(x^t |Φ)=∇_{m_i}\\sum_t\\sum_i h^t_i\\frac{(\\frac{-1}{2})(x^t -m_i)^T\\Sigma^{-1}(x^t-m_i)}{2\\pi^{\\frac{N}{2}}|\\Sigma|^{\\frac{1}{2}}} $\n",
        "\n",
        "By differentiating w.r.t $\\mu$ we get\n",
        "\n",
        "$\\Sigma_th^t_i(x^t-m_i)=0$\n",
        "\n",
        "$m^{l+1}_i=\\frac{\\sum_t h^t_ix^t}{\\sum_t h^t_i}$ ..............(6)\n",
        "\n",
        "By differentiating w.r.t $ S $(COVARIANCE)  we get\n",
        "\n",
        "$∇_{\\Sigma}\\sum_t\\sum_i h^t_i \\log p_i(x^t |Φ)=∇_{\\Sigma}\\sum_t\\sum_i h^t_ilog(\\frac{exp^{(\\frac{-1}{2})(x^t -m_i)^T\\Sigma^{-1}(x^t-m_i)}}{2\\pi^{\\frac{N}{2}}|\\Sigma|^{\\frac{1}{2}}})$\n",
        "\n",
        "$=∇_{\\Sigma}\\sum_t\\sum_i h^t_i [\\frac{-1}{2})(x^t -m_i)^T\\Sigma^{-1}(x^t-m_i)]+\\Sigma_ih^t_i {\\frac{-1}{2}} log[|\\Sigma|] =0$\n",
        "\n",
        "\n",
        "\n",
        "$S^{l+1}_i=\\frac{\\sum^t h^t_i(x^t −m^{l+1}_i )(x^t −m^{l+1}_i )^T}{\\sum_t h^t_i }$ ..............(7)\n",
        "\n",
        "where, for Gaussian components in the E-step, we calculate\n",
        "\n",
        "$h^t_i=\\frac{π_i|S_i|^{−1/2 }exp[−(1/2)(x^t −m_i)^T S^{−1}_i (x^t −m_i)]} {\\sum_j π_j |S_j |^{−1/2} exp[−(1/(x^t−m_j)^T S^{−1}_j (x^t −m_j)]}$ .........(8)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YUSuSoksktT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DIFFERENT SCENARIOS\n",
        "\n",
        "***I. Shared COVARIANCE MATRIX***\n",
        "\n",
        "Lagrangian multiplier for maximisation becomes\n",
        "\n",
        "$∇_{\\Sigma}\\sum_t\\sum_i h^t_i [\\frac{-1}{2})(x^t -m_i)^T\\Sigma^{-1}(x^t-m_i)] =0$\n",
        "\n",
        "$∇_{\\Sigma}\\sum_t\\sum_i h^t_i [\\frac{-1}{2})(x^t -m_i)^T\\S^{-1}(x^t-m_i)] =0$\n",
        "\n",
        "\n",
        "***II. Shared diagonal covariance Matrix***\n",
        "\n",
        "Lagrangian multiplier for maximisation becomes\n",
        "\n",
        "\n",
        "$∇_{\\Sigma}\\sum_t\\sum_i h^t_i [\\frac{-1}{2})(x^t -m_i)^T\\Sigma^{-1}(x^t-m_i)] =0$\n",
        "\n",
        "$∇_{\\Sigma}\\sum_t\\sum_i h^t_i [\\frac{-1}{2})(x^t -m_i)^T\\ S^{-1}(x^t-m_i)] =0$\n",
        "\n",
        "$∇_{\\Sigma}\\sum_t\\sum_i h^t_i [\\frac{-1}{2})(x^t -m_i)^2\\ S^{-1}] =0$\n",
        "\n",
        "$∇_{\\Sigma}\\sum_t\\sum_i h^t_i [\\frac{||x^t -m_i||^2}{s^2})] =0$\n",
        "\n",
        "Estimation of Z becomes\n",
        "\n",
        "\n",
        "$h^t_i= \\frac{exp[−(1/2s^2)||x_t −m_i||^2]}{\\Sigma_j exp[−(1/2s^2)||x_t −m_j||^2]}$\n",
        "\n",
        "***Shapes of Clusters***\n",
        "\n",
        "1. Kmeans cluster shapes are circular\n",
        "2. EM algorithmn cluster shapes are ellipses of arbitary shapes,orientation and coverage\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5knBoQxw1os_"
      }
    }
  ]
}